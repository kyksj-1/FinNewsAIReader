import aiohttp
import asyncio
import feedparser
import time
import json
from typing import List, Set
from loguru import logger

class NewsMonitor:
    """
    é›·è¾¾æ¨¡å— v4 (Ultimate): RSSçŸ©é˜µ + APIç›´è¿
    """
    def __init__(self):
        self.seen_urls: Set[str] = set()
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        # === æ ¸å¿ƒèµ„äº§ï¼šè¿™æ˜¯ä½ è¦çš„â€œè¶³å¤Ÿå¤šçš„ç½‘ç«™â€ ===
        # è¿™äº›æ˜¯æ ‡å‡† RSS é“¾æ¥ï¼Œç›´æ¥èƒ½ç”¨ï¼Œæ— éœ€æ’ä»¶
        self.rss_sources = [
            # 1. 21ä¸–çºªç»æµæŠ¥é“ (æœ€ç¨³çš„å®è§‚/é‡‘èæº)
            "http://www.21jingji.com/rss/21jingji/finance.xml", 
            "http://www.21jingji.com/rss/21jingji/macro.xml",
            
            # 2. è´¢æ–°ç½‘ (è™½ç„¶æœ‰ä»˜è´¹å¢™ï¼Œä½†RSSèƒ½æŠ“åˆ°æ‘˜è¦å’Œé“¾æ¥ï¼Œéƒ¨åˆ†å…è´¹)
            "http://ma.caixin.com/rss/finance.xml",
            
            # 3. è™å—… (ç§‘æŠ€ä¸å•†ä¸šï¼Œè´¨é‡å¾ˆé«˜)
            "https://www.huxiu.com/rss/0.xml",
            
            # 4. 36æ°ª (ä¸€çº§å¸‚åœºã€TMT)
            "https://www.36kr.com/feed",
            
            # 5. FTä¸­æ–‡ç½‘ (å…¨çƒè§†é‡)
            "http://www.ftchinese.com/rss/news",
            "http://www.ftchinese.com/rss/markets",
            
            # 6. ç•Œé¢æ–°é—» (é€šè¿‡ RSS æŠ“æ¯”çˆ¬ç½‘é¡µæ›´ç¨³)
            "https://a.jiemian.com/index.php?m=article&a=rss&cid=4", # ç•Œé¢-è¯åˆ¸
            
            # 7. æœç‹è´¢ç» (è€ç‰Œï¼Œé‡å¤§)
            "http://business.sohu.com/rss/scroll.xml",
            
            # 8. è”åˆæ—©æŠ¥-è´¢ç» (äºšå¤ªè§†è§’)
            "https://www.zaobao.com.sg/finance/rss.xml",
            
            # 9. æ™ºé€šè´¢ç» (æ¸¯ç¾è‚¡)
            "https://www.zhitongcaijing.com/rss.xml",
            
            # 10. åå°”è¡—è§é—» (æ³¨æ„ï¼šè¿™æ˜¯ç¬¬ä¸‰æ–¹ç»´æŠ¤çš„æºï¼Œå¦‚æœå¤±æ•ˆå¯ä»¥åˆ æ‰)
            # è¿™ç§ rsshub å¼€å¤´çš„å¦‚æœæœ¬åœ°æ²¡ç½‘å¯èƒ½è¿ä¸ä¸Šï¼Œä½ å¯ä»¥è¯•ä¸€ä¸‹
            "https://rsshub.app/wallstreetcn/news/global" 
        ]

    async def scan_rss_feed(self, url: str) -> List[str]:
        """
        é€šç”¨ RSS æ‰«æå™¨
        """
        new_links = []
        try:
            # feedparser æ˜¯åŒæ­¥ IOï¼Œä¸ºäº†ä¸é˜»å¡ä¸»å¾ªç¯ï¼Œä¸¢åˆ°çº¿ç¨‹æ± è¿è¡Œ
            feed = await asyncio.to_thread(feedparser.parse, url)
            
            if hasattr(feed, 'entries'):
                for entry in feed.entries:
                    link = entry.link
                    # ç®€å•è¿‡æ»¤ï¼šåªä¿ç•™ http å¼€å¤´çš„æœ‰æ•ˆé“¾æ¥
                    if link and link.startswith('http') and link not in self.seen_urls:
                        new_links.append(link)
                        self.seen_urls.add(link)
                        
        except Exception as e:
            # RSS å¶å°”è¿æ¥è¶…æ—¶å¾ˆæ­£å¸¸ï¼Œä¸ç”¨ print stack traceï¼Œå¤ªåµ
            logger.warning(f"RSS Feed requires check: {url} | {str(e)[:50]}")
        
        return new_links

    async def scan_sina_7x24(self, session: aiohttp.ClientSession) -> List[str]:
        # (ä¿ç•™ä½ ä¹‹å‰çš„ä»£ç ï¼Œè¿™æ˜¯ä¸ªå¥½æ¥å£)
        ts = int(time.time() * 1000)
        api_url = f"https://zhibo.sina.com.cn/api/zhibo/feed?callback=&page=1&page_size=20&zhibo_id=152&tag_id=0&dire=f&dpc=1&type=0&_={ts}"
        new_links = []
        try:
            async with session.get(api_url, headers=self.headers, timeout=10) as resp:
                text = await resp.text()
                data = json.loads(text)
                items = data.get('result', {}).get('data', {}).get('feed', {}).get('list', [])
                for item in items:
                    url = item.get('docurl')
                    if url and url not in self.seen_urls:
                        new_links.append(url)
                        self.seen_urls.add(url)
        except Exception: pass
        return new_links

    async def scan_eastmoney_kuaixun(self, session: aiohttp.ClientSession) -> List[str]:
        # (ä¿ç•™ä½ ä¹‹å‰çš„ä»£ç )
        api_url = "https://newsapi.eastmoney.com/kuaixun/v1/getlist_102_ajaxResult_50_1_.html"
        new_links = []
        try:
            async with session.get(api_url, headers=self.headers, timeout=10) as resp:
                text = await resp.text()
                try:
                    json_str = text[text.find('{'):text.rfind('}')+1]
                    data = json.loads(json_str)
                    for item in data.get('LivesList', []):
                        url = item.get('url_unique')
                        if url and url not in self.seen_urls:
                            new_links.append(url)
                            self.seen_urls.add(url)
                except: pass
        except Exception: pass
        return new_links

    async def harvest(self) -> List[str]:
        """
        å…¨ç«åŠ›è¦†ç›–æ‰«æ
        """
        async with aiohttp.ClientSession() as session:
            # 1. å¯åŠ¨ API ä»»åŠ¡
            tasks = [
                self.scan_sina_7x24(session),
                self.scan_eastmoney_kuaixun(session)
            ]
            
            # 2. å¯åŠ¨æ‰€æœ‰ RSS ä»»åŠ¡
            for rss_url in self.rss_sources:
                tasks.append(self.scan_rss_feed(rss_url))
            
            # 3. å¹¶å‘ç­‰å¾…
            results = await asyncio.gather(*tasks)
            
            # 4. å±•å¹³ç»“æœ
            all_urls = [u for sub in results for u in sub]
            
            # è°ƒè¯•æ‰“å°
            if all_urls:
                logger.info(f"ğŸ“¡ Radar Detected {len(all_urls)} URLs from {len(self.rss_sources) + 2} sources.")
            
            return all_urls