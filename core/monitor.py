import aiohttp
import asyncio
import time
import json
from typing import List, Set
from loguru import logger

class NewsMonitor:
    """
    é›·è¾¾æ¨¡å— v3 (Pro): èšåˆæ–°æµªã€ä¸œè´¢ã€ç•Œé¢
    """
    def __init__(self):
        self.seen_urls: Set[str] = set()
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://finance.sina.com.cn/"
        }

    async def scan_sina_7x24(self, session: aiohttp.ClientSession) -> List[str]:
        """
        æ–°æµªè´¢ç» 7x24 (Aè‚¡ä¿¡æ¯å¯†åº¦æœ€é«˜çš„åœ°æ–¹)
        """
        # æ–°æµªçš„ API æ—¶é—´æˆ³å‚æ•°
        ts = int(time.time() * 1000)
        api_url = f"https://zhibo.sina.com.cn/api/zhibo/feed?callback=&page=1&page_size=20&zhibo_id=152&tag_id=0&dire=f&dpc=1&type=0&_={ts}"
        new_links = []
        try:
            async with session.get(api_url, headers=self.headers, timeout=10) as resp:
                text = await resp.text()
                data = json.loads(text)
                items = data.get('result', {}).get('data', {}).get('feed', {}).get('list', [])
                
                for item in items:
                    # æ–°æµªå¿«è®¯å¾€å¾€æ²¡æœ‰ç‹¬ç«‹ URLï¼Œåªæœ‰ docurl
                    url = item.get('docurl')
                    # å¦‚æœæ²¡æœ‰ docurlï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ªä¼ª ID URL é˜²æ­¢é‡å¤å¤„ç†
                    if not url and item.get('id'):
                        # å¯¹äºçº¯å¿«è®¯ï¼ˆæ— æ–‡ç« ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥æ„é€ ä¸€ä¸ª text payload ä¼ ç»™ä¸‹æ¸¸
                        # ä½†ä¸ºäº†ä¿æŒæ¶æ„ç»Ÿä¸€ï¼Œæˆ‘ä»¬è¿™é‡Œæš‚æ—¶åªå–æœ‰è¯¦æƒ…é¡µçš„
                        pass 
                    
                    if url and url not in self.seen_urls:
                        new_links.append(url)
                        self.seen_urls.add(url)
        except Exception as e:
            logger.error(f"Sina Scan Error: {e}")
        return new_links

    async def scan_eastmoney_kuaixun(self, session: aiohttp.ClientSession) -> List[str]:
        """
        ä¸œæ–¹è´¢å¯Œ 7x24
        """
        api_url = "https://newsapi.eastmoney.com/kuaixun/v1/getlist_102_ajaxResult_50_1_.html"
        new_links = []
        try:
            async with session.get(api_url, headers=self.headers, timeout=10) as resp:
                text = await resp.text()
                # æš´åŠ›è§£æ var ajaxResult = {...}
                try:
                    json_str = text[text.find('{'):text.rfind('}')+1]
                    data = json.loads(json_str)
                    for item in data.get('LivesList', []):
                        url = item.get('url_unique')
                        if url and url not in self.seen_urls:
                            new_links.append(url)
                            self.seen_urls.add(url)
                except: pass
        except Exception: pass
        return new_links

    async def scan_jiemian(self, session: aiohttp.ClientSession) -> List[str]:
        """
        ç•Œé¢æ–°é—» (æ·±åº¦æŠ¥é“)
        """
        # ... (ä¿æŒä¹‹å‰çš„é€»è¾‘ï¼Œç•¥ï¼Œè¯·ä¿ç•™åŸæœ¬çš„ BeautifulSoup ä»£ç ) ...
        # è¿™é‡Œä¸ºäº†èŠ‚çœç¯‡å¹…ï¼Œå‡è®¾ä½ ä¿ç•™äº†ä¹‹å‰çš„ scan_jiemian ä»£ç 
        return [] 

    async def harvest(self) -> List[str]:
        async with aiohttp.ClientSession() as session:
            tasks = [
                self.scan_sina_7x24(session),
                self.scan_eastmoney_kuaixun(session),
                # self.scan_jiemian(session) # è®°å¾—åŠ ä¸Šè¿™ä¸ª
            ]
            results = await asyncio.gather(*tasks)
            all_urls = [u for sub in results for u in sub]
            
            # å»é‡è¿‡æ»¤
            valid_urls = [u for u in all_urls if u.startswith("http")]
            
            if valid_urls:
                logger.info(f"ğŸ“¡ Radar V3 detected {len(valid_urls)} signals.")
            return valid_urls