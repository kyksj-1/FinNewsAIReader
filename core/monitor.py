import aiohttp
import asyncio
import feedparser
import time
import json
from typing import List, Set
from loguru import logger

class NewsMonitor:
    """
    é›·è¾¾æ¨¡å— v5: å¢åŠ ç›´æ¥APIæŠ“å– + ç»Ÿè®¡åŠŸèƒ½
    """
    def __init__(self):
        self.seen_urls: Set[str] = set()
        self.stats = {
            'total_scanned': 0,
            'new_urls': 0,
            'rss_success': 0,
            'rss_failed': 0
        }
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        # ===== æ–°å¢: ç›´æ¥APIæº ===== 
        # è¿™äº›è¿”å›JSON,ä¸ç»è¿‡RSS, ç›´æ¥ä½œä¸ºURLäº¤ç»™Crawlerçš„JSONè§£æå™¨å¤„ç†
        self.api_sources = [
            # ä¸œæ–¹è´¢å¯Œå¿«è®¯ (æ¯æ¬¡è¿”å›50æ¡)
            "https://newsapi.eastmoney.com/kuaixun/v1/getlist_102_ajaxResult_50_1_.html",
            
            # æ–°æµªè´¢ç»7x24 (å¯è°ƒæ•´page_size)
            "https://zhibo.sina.com.cn/api/zhibo/feed?page=1&page_size=30&zhibo_id=152",
        ]

        # === æ ¸å¿ƒèµ„äº§ï¼šè¿™æ˜¯ä½ è¦çš„â€œè¶³å¤Ÿå¤šçš„ç½‘ç«™â€ ===
        # è¿™äº›æ˜¯æ ‡å‡† RSS é“¾æ¥ï¼Œç›´æ¥èƒ½ç”¨ï¼Œæ— éœ€æ’ä»¶
        self.rss_sources = [
            # 1. 21ä¸–çºªç»æµæŠ¥é“ (æœ€ç¨³çš„å®è§‚/é‡‘èæº)
            "http://www.21jingji.com/rss/21jingji/finance.xml", 
            "http://www.21jingji.com/rss/21jingji/macro.xml",
            
            # 2. è´¢æ–°ç½‘ (è™½ç„¶æœ‰ä»˜è´¹å¢™ï¼Œä½†RSSèƒ½æŠ“åˆ°æ‘˜è¦å’Œé“¾æ¥ï¼Œéƒ¨åˆ†å…è´¹)
            "http://ma.caixin.com/rss/finance.xml",
            
            # 3. è™å—… (ç§‘æŠ€ä¸å•†ä¸šï¼Œè´¨é‡å¾ˆé«˜)
            "https://www.huxiu.com/rss/0.xml",
            
            # 4. 36æ°ª (ä¸€çº§å¸‚åœºã€TMT)
            "https://www.36kr.com/feed",
            
            # 5. FTä¸­æ–‡ç½‘ (å…¨çƒè§†é‡)
            "http://www.ftchinese.com/rss/news",
            "http://www.ftchinese.com/rss/markets",
            
            # 6. ç•Œé¢æ–°é—» (é€šè¿‡ RSS æŠ“æ¯”çˆ¬ç½‘é¡µæ›´ç¨³)
            "https://a.jiemian.com/index.php?m=article&a=rss&cid=4", # ç•Œé¢-è¯åˆ¸
            
            # 7. æœç‹è´¢ç» (è€ç‰Œï¼Œé‡å¤§)
            "http://business.sohu.com/rss/scroll.xml",
            
            # 8. è”åˆæ—©æŠ¥-è´¢ç» (äºšå¤ªè§†è§’)
            "https://www.zaobao.com.sg/finance/rss.xml",
            
            # 9. æ™ºé€šè´¢ç» (æ¸¯ç¾è‚¡)
            "https://www.zhitongcaijing.com/rss.xml",
            
            # 10. åå°”è¡—è§é—» (æ³¨æ„ï¼šè¿™æ˜¯ç¬¬ä¸‰æ–¹ç»´æŠ¤çš„æºï¼Œå¦‚æœå¤±æ•ˆå¯ä»¥åˆ æ‰)
            # è¿™ç§ rsshub å¼€å¤´çš„å¦‚æœæœ¬åœ°æ²¡ç½‘å¯èƒ½è¿ä¸ä¸Šï¼Œä½ å¯ä»¥è¯•ä¸€ä¸‹
            "https://rsshub.app/wallstreetcn/news/global" 
        ]

    async def scan_rss_feed(self, url: str) -> List[str]:
        """
        é€šç”¨ RSS æ‰«æå™¨
        """
        new_links = []
        try:
            # feedparser æ˜¯åŒæ­¥ IOï¼Œä¸ºäº†ä¸é˜»å¡ä¸»å¾ªç¯ï¼Œä¸¢åˆ°çº¿ç¨‹æ± è¿è¡Œ
            feed = await asyncio.to_thread(feedparser.parse, url)
            
            if hasattr(feed, 'entries'):
                for entry in feed.entries:
                    link = entry.link
                    # ç®€å•è¿‡æ»¤ï¼šåªä¿ç•™ http å¼€å¤´çš„æœ‰æ•ˆé“¾æ¥
                    if link and link.startswith('http') and link not in self.seen_urls:
                        new_links.append(link)
                        self.seen_urls.add(link)
                
                if new_links:
                    self.stats['rss_success'] += 1
                        
        except Exception as e:
            self.stats['rss_failed'] += 1
            # RSS å¶å°”è¿æ¥è¶…æ—¶å¾ˆæ­£å¸¸ï¼Œä¸ç”¨ print stack traceï¼Œå¤ªåµ
            logger.warning(f"RSS Feed requires check: {url} | {str(e)[:50]}")
        
        return new_links

    async def scan_api_endpoint(self, session: aiohttp.ClientSession, api_url: str) -> List[str]:
        """
        æ‰«æè¿”å›JSONçš„APIæ¥å£
        è¿”å›çš„ä¸æ˜¯URLåˆ—è¡¨,è€Œæ˜¯ç›´æ¥æŠŠAPIåœ°å€åŠ å…¥é˜Ÿåˆ—
        (å› ä¸ºè¿™äº›APIæœ¬èº«å°±æ˜¯æ•°æ®æº)
        """
        try:
            async with session.get(api_url, headers=self.headers, timeout=10) as resp:
                if resp.status == 200:
                    # å¯¹äºAPI URLï¼Œæˆ‘ä»¬ä¸æ ¹æ®å†…å®¹å»é‡ï¼ˆå› ä¸ºå†…å®¹ä¼šå˜ï¼‰ï¼Œè€Œæ˜¯æ€»æ˜¯å…è®¸å®ƒè¢«å¤„ç†
                    # ä½†æ˜¯ä¸ºäº†é˜²æ­¢ pipeline è¿‡äºæ‹¥å µï¼Œå¯ä»¥åšä¸€ä¸ªç®€å•çš„é¢‘ç‡é™åˆ¶æˆ– hash check (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæ€»æ˜¯è¿”å›)
                    # å®é™…ä¸Šï¼Œå¦‚æœ API URL æœ¬èº«ä¸å˜ï¼Œseen_urls æœºåˆ¶ä¼šæ‹¦æˆªå®ƒã€‚
                    # æ‰€ä»¥è¿™é‡Œæœ‰ä¸€ä¸ªç‰¹æ®Šé€»è¾‘ï¼šAPI URL åº”è¯¥è¢«è§†ä¸ºâ€œç”Ÿæˆå™¨â€ï¼Œè€Œä¸æ˜¯â€œæ–‡ç« â€ã€‚
                    # ä½†æ˜¯ FinNewsPipeline çš„è®¾è®¡æ˜¯ URL -> Processã€‚
                    # ä¸ºäº†è®© Crawler æ¯æ¬¡éƒ½å»æŠ“æ–°çš„ JSONï¼Œæˆ‘ä»¬éœ€è¦è®© Monitor æ¯æ¬¡éƒ½æŠŠè¿™ä¸ª API URL æŠ›å‡ºå»å—ï¼Ÿ
                    # ä¸ï¼Œå¦‚æœ seen_urls è®°å½•äº† api_urlï¼Œä¸‹æ¬¡å°±ä¸æŠ“äº†ã€‚
                    # **ä¿®æ­£**: API URL ä¸åº”è¯¥åŠ å…¥ seen_urlsï¼Œæˆ–è€…æ¯æ¬¡åŠ ä¸€ä¸ªæ—¶é—´æˆ³å‚æ•°è®©å®ƒä¸åŒã€‚
                    
                    # ç­–ç•¥ï¼šMonitor è¿”å› API URLï¼ŒCrawler è§£æå‡º NewsItemsã€‚
                    # æˆ‘ä»¬éœ€è¦ç¡®ä¿ Monitor æ¯æ¬¡éƒ½èƒ½æŠŠ API URL æŠ¥ä¸Šå»ã€‚
                    return [api_url] 
        except Exception as e:
            logger.debug(f"API scan skip: {api_url[:40]}... ({str(e)[:20]})")
        
        return []

    async def harvest(self) -> List[str]:
        """
        å…¨ç«åŠ›æ‰«æ + ç»Ÿè®¡æŠ¥å‘Š
        """
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            # 1. å¯åŠ¨ API ä»»åŠ¡ (ç›´æ¥æŠŠ API URL äº¤ç»™ Crawler å¤„ç†)
            for api_url in self.api_sources:
                tasks.append(self.scan_api_endpoint(session, api_url))
            
            # 2. å¯åŠ¨æ‰€æœ‰ RSS ä»»åŠ¡
            for rss_url in self.rss_sources:
                tasks.append(self.scan_rss_feed(rss_url))
            
            # 3. å¹¶å‘ç­‰å¾…
            results = await asyncio.gather(*tasks)
            
            # 4. å±•å¹³ç»“æœ
            all_urls = [u for sub in results for u in sub]
            
            # ç»Ÿè®¡
            self.stats['total_scanned'] += 1
            self.stats['new_urls'] += len(all_urls)
            
            # æ¯10æ¬¡æ‰«ææ‰“å°ç»Ÿè®¡
            if self.stats['total_scanned'] % 10 == 0:
                logger.info(
                    f"ğŸ“Š Scan Stats: "
                    f"Total={self.stats['total_scanned']} | "
                    f"NewURLs={self.stats['new_urls']} | "
                    f"RSS_OK={self.stats['rss_success']} | "
                    f"RSS_Fail={self.stats['rss_failed']}"
                )
            
            if all_urls:
                logger.info(f"ğŸ“¡ Detected {len(all_urls)} URLs this round")
            
            return all_urls