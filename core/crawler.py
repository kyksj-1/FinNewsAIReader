import aiohttp
import asyncio
import json
from typing import Optional, List, Union
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from loguru import logger
from config.settings import settings
from core.schema import NewsPayload

class AsyncCrawler:
    """
    å¼‚æ­¥æ•°æ®é‡‡é›†å™¨ - å¢å¼ºç‰ˆ
    æ”¯æŒHTMLå’ŒJSONä¸¤ç§æ•°æ®æº
    """
    def __init__(self):
        self.semaphore = asyncio.Semaphore(settings.MAX_CRAWLER_CONCURRENCY)
        self.headers = {
            "User-Agent": "FinNewsMasterV1/1.0 (Quant Research; SJTU Physics)"
        }

    def _is_json_api(self, url: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºJSON API (è€Œéæ™®é€šç½‘é¡µ)
        """
        json_indicators = [
            'api.eastmoney.com',
            'newsapi.eastmoney.com',
            'zhibo.sina.com.cn/api',
            '/api/',
            'getlist',
            'ajaxResult'
        ]
        return any(indicator in url for indicator in json_indicators)

    async def fetch_json_api(self, session: aiohttp.ClientSession, url: str) -> List[NewsPayload]:
        """
        ç›´æ¥è§£æJSON APIè¿”å›çš„å¿«è®¯æ•°æ®
        è¿”å› NewsPayload åˆ—è¡¨
        """
        news_list = []
        async with self.semaphore:
            try:
                # API é€šå¸¸å“åº”å¿«ï¼Œä¸éœ€è¦å¤ªé•¿ timeout
                async with session.get(url, headers=self.headers, timeout=15) as response:
                    response.raise_for_status()
                    text = await response.text()
                    
                    # ä¸œæ–¹è´¢å¯Œçš„æ•°æ®å¯èƒ½åŒ…è£¹åœ¨ var ajaxResult={...} ä¸­
                    if 'var ajaxResult=' in text:
                        json_str = text.split('var ajaxResult=')[1].strip()
                        if json_str.endswith(';'):
                            json_str = json_str[:-1]
                    else:
                        json_str = text
                    
                    try:
                        data = json.loads(json_str)
                    except json.JSONDecodeError:
                        # å°è¯•åªæˆªå– {} éƒ¨åˆ†
                        start = text.find('{')
                        end = text.rfind('}') + 1
                        if start != -1 and end != -1:
                            data = json.loads(text[start:end])
                        else:
                            raise

                    # 1. è§£æä¸œæ–¹è´¢å¯Œå¿«è®¯æ ¼å¼
                    if 'LivesList' in data:
                        items = data['LivesList']
                        for item in items:
                            payload = NewsPayload(
                                url=item.get('url_unique', url), # å¦‚æœæ²¡æœ‰ç‹¬ç«‹URLï¼Œå°±ç”¨API URLä½œä¸ºå ä½ï¼Œæˆ–è€…æ„é€ ä¸€ä¸ªå”¯ä¸€ID
                                title=item.get('simtitle', item.get('title', 'Unknown')),
                                content=item.get('digest', item.get('simdigest', '')),
                                source='EastMoney_API'
                            )
                            news_list.append(payload)
                            
                    # 2. è§£ææ–°æµªè´¢ç» 7x24 æ ¼å¼
                    elif 'result' in data and 'data' in data['result']:
                         items = data['result']['data']['feed']['list']
                         for item in items:
                             payload = NewsPayload(
                                 url=item.get('docurl', url),
                                 title=item.get('rich_text', item.get('plain_text', 'Unknown'))[:50], # æ–°æµªå¿«è®¯å¾€å¾€æ²¡æœ‰æ ‡é¢˜ï¼Œæˆªå–å†…å®¹å‰æ®µ
                                 content=item.get('rich_text', item.get('plain_text', '')),
                                 source='Sina_API'
                             )
                             news_list.append(payload)

                    logger.info(f"ğŸ“¦ Parsed {len(news_list)} news items from JSON API: {url[:30]}...")
                    return news_list
                    
            except Exception as e:
                logger.error(f"JSON API parse error for {url}: {e}")
                return []

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(aiohttp.ClientError)
    )
    async def fetch_jina_markdown(self, session: aiohttp.ClientSession, url: str) -> str:
        target_url = f"{settings.JINA_READER_BASE}{url}"
        async with self.semaphore:
            # ä¿®æ”¹ç‚¹ï¼štimeout ä» 15 æ”¹æˆ 30
            async with session.get(target_url, headers=self.headers, timeout=30) as response:
                response.raise_for_status()
                return await response.text()

    async def process_url(self, url: str) -> Union[NewsPayload, List[NewsPayload], None]:
        """
        å•ä¸€ URL å¤„ç†æµç¨‹ - è‡ªåŠ¨è¯†åˆ«JSON/HTML
        """
        async with aiohttp.ClientSession() as session:
            try:
                # 1. å¦‚æœæ˜¯JSON API,ç›´æ¥è§£æ
                if self._is_json_api(url):
                    logger.info(f"ğŸ” Detected JSON API: {url[:50]}...")
                    return await self.fetch_json_api(session, url)

                # 2. å¦åˆ™èµ°Jina Reader (æ™®é€šç½‘é¡µ)
                logger.info(f"Downloading signal: {url}")
                content = await self.fetch_jina_markdown(session, url)
                
                # ç®€å•æå–æ ‡é¢˜ (Jina è¿”å›çš„ Markdown ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯æ ‡é¢˜)
                lines = content.split('\n')
                title = lines[0].strip('# ').strip() if lines else "Unknown Title"
                
                return NewsPayload(
                    url=url,
                    title=title,
                    content=content
                )
            except Exception as e:
                logger.error(f"Signal Loss for {url}: {e}")
                return None
