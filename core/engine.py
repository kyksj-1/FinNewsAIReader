import aiohttp
import asyncio
import json
import re
import subprocess
import numpy as np
from typing import Optional, List
from loguru import logger
from config.settings import settings
from core.schema import NewsPayload, SignalAnalysis


class LLMEngine:
    """
    æ¨ç†å¼•æ“ï¼šæ”¯æŒæœ¬åœ° Ollama å’Œäº‘ç«¯ DeepSeek
    """

    def __init__(self):
        limit = 50 if settings.LLM_PROVIDER == "deepseek" else settings.MAX_GPU_CONCURRENCY
        self.concurrency_lock = asyncio.Semaphore(limit)

    async def _get_gpu_temperature(self) -> Optional[int]:
        def runner() -> Optional[int]:
            try:
                result = subprocess.run(
                    [
                        "nvidia-smi",
                        "--query-gpu=temperature.gpu",
                        "--format=csv,noheader,nounits",
                    ],
                    capture_output=True,
                    text=True,
                    check=True,
                )
                line = result.stdout.strip().splitlines()[0].strip()
                return int(line)
            except Exception:
                return None

        return await asyncio.to_thread(runner)

    async def _wait_for_safe_temperature(self) -> None:
        limit = settings.GPU_TEMP_LIMIT
        resume = settings.GPU_TEMP_RESUME
        interval = settings.GPU_TEMP_CHECK_INTERVAL
        if limit <= 0 or interval <= 0:
            return
        if resume <= 0 or resume >= limit:
            resume = max(limit - 10, 0)
        temp = await self._get_gpu_temperature()
        if temp is None:
            return
        if temp < limit:
            return
        logger.warning(f"GPU temperature {temp}Â°C exceeds limit {limit}Â°C, waiting for cooldown")
        while True:
            await asyncio.sleep(interval)
            temp = await self._get_gpu_temperature()
            if temp is None:
                logger.warning("GPU temperature check failed during cooldown, resuming inference")
                return
            if temp <= resume:
                logger.info(f"GPU temperature {temp}Â°C is below resume threshold {resume}Â°C, resuming inference")
                return

    async def _call_deepseek(self, prompt: str, temp: float, max_tokens: int) -> str:
        """
        DeepSeek API è°ƒç”¨ (OpenAI å…¼å®¹åè®®)
        """
        url = f"{settings.DEEPSEEK_BASE_URL}/chat/completions"
        headers = {
            "Authorization": f"Bearer {settings.DEEPSEEK_API_KEY}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": settings.DEEPSEEK_MODEL_NAME,
            "messages": [
                {"role": "system", "content": "You are a professional financial quantitative analyst."},
                {"role": "user", "content": prompt},
            ],
            "temperature": temp,
            "max_tokens": max_tokens,
            "stream": False,
        }
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(url, headers=headers, json=payload, timeout=120) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        logger.error(f"[DeepSeek] Error {resp.status}: {error_text}")
                        return ""
                    data = await resp.json()
                    return data["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error(f"[DeepSeek] Connection Failed: {e}")
                return ""

    async def _call_ollama(self, prompt: str, temp: float, max_tokens: int) -> str:
        """
        æœ¬åœ° Ollama è°ƒç”¨
        """
        url = f"{settings.OLLAMA_BASE_URL}/api/generate"
        payload = {
            "model": settings.LOCAL_MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temp,
                "num_ctx": settings.CONTEXT_WINDOW,
                "num_predict": max_tokens,
            },
        }
        await self._wait_for_safe_temperature()
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(url, json=payload, timeout=60) as resp:
                    resp.raise_for_status()
                    data = await resp.json()
                    return data.get("response", "")
            except Exception as e:
                logger.error(f"[Ollama] Error: {e}")
                return ""

    async def call_model(self, prompt: str, temp: float, max_tokens: int = 2048) -> str:
        """
        ç»Ÿä¸€å…¥å£ï¼šæ ¹æ®é…ç½®åˆ†å‘
        """
        async with self.concurrency_lock:
            if settings.LLM_PROVIDER == "deepseek":
                return await self._call_deepseek(prompt, temp, max_tokens)
            return await self._call_ollama(prompt, temp, max_tokens)

    async def fast_path_filter(self, news: NewsPayload) -> bool:
        """
        å¿«é€šé“ï¼šåŸºäºæ ‡é¢˜çš„å¿«é€ŸäºŒåˆ†ç±» (High-Pass Filter)
        ä¿®å¤ç‰ˆï¼šä½¿ç”¨ä¸­æ–‡ Promptï¼Œé™ä½é˜ˆå€¼ï¼Œå¢åŠ  Debug æ—¥å¿—
        """
        # 1. æç®€è§„åˆ™ï¼šå¦‚æœæ ‡é¢˜åŒ…å«ç‰¹å®šç¡¬å…³é”®è¯ï¼Œç›´æ¥é€šè¿‡ï¼ˆæ—è·¯æœºåˆ¶ï¼‰
        # ç‰©ç†ç›´è§‰ï¼šæœ‰äº›ä¿¡å·å¤ªæ˜æ˜¾ï¼Œä¸éœ€è¦è¿‡æ¨¡å‹
        keywords = ["Aè‚¡", "è‚¡å¸‚", "äººæ°‘å¸", "å¤®è¡Œ", "ç¾è”å‚¨", "åˆ©å¥½", "åˆ©ç©º", "GDP", "CPI", "ç›‘ç®¡"
        "èŠ¯ç‰‡", "åŠå¯¼ä½“", "è´¢æŠ¥", "å¢æŒ", "å›è´­", "AI", "é‡‘è", "ç®—åŠ›", "åŠå¯¼ä½“",
        "æ²ªæŒ‡", "æ¿å—", "æ¦‚å¿µè‚¡", "è‚¡ç¥¨", "æ¶¨åœ", "è·Œåœ", "å›è°ƒ", "åå¼¹", "å¸‚åœºæƒ…ç»ª",
        "èèµ„", "è¯åˆ¸", "å¤§ç›˜", "æŒ‡æ•°", "æˆäº¤é¢", "åŒ—å‘", "å¤–èµ„", "ç‰¹æ–¯æ‹‰", "å®å¾·æ—¶ä»£"
        ]
        if any(k in news.title for k in keywords):
            logger.info(f"âš¡ [Fast Path] Keyword Bypass | {news.title[:60]}...")
            return True

        # 2. LLM åˆ¤åˆ«
        prompt = f"""
        åˆ¤æ–­æ ‡é¢˜æ˜¯å¦ä¸"é‡‘è/ç»æµ/ç§‘æŠ€"ç›¸å…³:
        {news.title}
        
        åªå›ç­”"æ˜¯"æˆ–"å¦",ä¸€ä¸ªå­—ã€‚
        """
        
        # é™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§
        res = await self.call_model(prompt, temp=0.1, max_tokens=10)
        clean_res = res.strip().upper()
        
        # ä¿®æ”¹ç‚¹ï¼šæ‰“å°åŸå§‹å›å¤ï¼Œçœ‹çœ‹å®ƒåˆ°åº•æƒ³è¯´ä»€ä¹ˆ
        logger.debug(f"Raw Model Response: {clean_res}")
        
        # 3. å®½æ¾åˆ¤åˆ«é€»è¾‘
        is_relevant = (
            "æ˜¯" in clean_res or 
            "YES" in clean_res or 
            "ç›¸å…³" in clean_res or
            "Y" == clean_res or
            "TRUE" in clean_res
        )
        
        status = "Relevant" if is_relevant else "Noise"
        # å…³é”®ï¼šæ‰“å°å‡ºæ¨¡å‹åˆ°åº•è¯´äº†ä»€ä¹ˆï¼Œæ–¹ä¾¿è°ƒè¯•
        logger.info(
            f"ğŸ” [Fast Path] Model said: '{clean_res}' -> {status} | Title: {news.title[:30]}..."
        )
        return is_relevant

    async def _single_analyze(self, news: NewsPayload, temp: float) -> Optional[SignalAnalysis]:
        """
        å•æ¬¡æ·±åº¦åˆ†æ
        """
        max_tokens_limit = 4096 
        safe_content = news.content[:6000] if news.content else ""
        
        prompt = f"""
        [Role]
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±é‡åŒ–ç ”ç©¶å‘˜ã€‚ä½ éœ€è¦åˆ†ææ–°é—»å¯¹Aè‚¡å¸‚åœºçš„å½±å“ã€‚å½“è®¯æ¯ä¸­å‡ºç°è‚¡ç¥¨åå­—çš„æ—¶å€™ï¼Œå¿…é¡»æ ¼å¤–æ³¨æ„ï¼è¯´æ˜è¿™ä¸ªè‚¡ç¥¨æ˜¯æœ‰æ¶ˆæ¯çš„ã€‚

        [Input News]
        {safe_content}

        [Instructions]
        1. **Deep Thinking (å…³é”®æ­¥éª¤)**: 
           åœ¨è¾“å‡º JSON ä¹‹å‰ï¼Œå¿…é¡»å…ˆåœ¨ä¸€ä¸ª <think> æ ‡ç­¾å†…è¿›è¡Œæ·±åº¦æ¨æ¼”ã€‚
           - åˆ†æäº‹ä»¶çš„ä¸€é˜¶å½±å“ï¼ˆç›´æ¥å—ç›Š/å—æŸï¼‰ã€‚
           - åˆ†æäºŒé˜¶å½±å“ï¼ˆä¾›åº”é“¾ã€ç«äº‰å¯¹æ‰‹ã€æ›¿ä»£å“ï¼‰ã€‚
           - ç»“åˆå½“å‰å®è§‚ç¯å¢ƒï¼ˆæµåŠ¨æ€§ã€æ”¿ç­–å‘¨æœŸï¼‰è¯„ä¼°ä¿¡å·å¼ºåº¦ã€‚
           - æ¨å¯¼æœ€ç»ˆçš„ Scoreã€‚

        2. **å¼ºåˆ¶è‡ªæˆ‘æ ¡éªŒ**:
           åœ¨ç»™å‡ºæœ€ç»ˆscoreå‰,å¿…é¡»å›ç­”:
           - è¿™ä¸ªscoreæ˜¯å¦è¿‡åº¦ä¾èµ–å•ä¸€ä¿¡æ¯æº?
           - è‹¥å…³é”®å‡è®¾ä¸æˆç«‹,scoreä¼šé™åˆ°å¤šå°‘?
           - å†å²ä¸Šç±»ä¼¼äº‹ä»¶çš„å®é™…å¸‚åœºååº”æ˜¯?

        3. **Confidence Interval**:
           é™¤äº†ç»™å‡ºscore,è¿˜è¦ç»™å‡º90%ç½®ä¿¡åŒºé—´ã€‚

        4. **Output Format**:
           æ€è€ƒç»“æŸåï¼Œè¾“å‡ºä¸¥æ ¼çš„ JSONã€‚
           
        [Example Output]
        <think>
        è¿™é‡Œå†™ä½ çš„æ·±åº¦æ¨ç†è¿‡ç¨‹...
        1. äº‹ä»¶æ ¸å¿ƒæ˜¯...
        2. ä¼ å¯¼è·¯å¾„æ˜¯...
        3. å¸‚åœºé¢„æœŸåœ¨äº...
        4. è‡ªæˆ‘æ ¡éªŒï¼šè¯¥ä¿¡å·ä¾èµ–...è‹¥...åˆ™...
        </think>
        {{
            "reasoning": "æ€»ç»“ä¸Šè¿°æ€è€ƒçš„ç®€ç»ƒç»“è®º...",
            "score": 7,
            "certainty": 8,
            "confidence_range": [5, 8],
            "related_stocks": ["sh.600XXX"],
            "time_horizon": "Medium"
        }}
        """
        
        raw_res = await self.call_model(prompt, temp=temp, max_tokens=max_tokens_limit)
        raw_res = raw_res.replace("```json", "").replace("```", "")
        try:
            match = re.search(r"\{.*\}", raw_res, re.DOTALL)
            if not match:
                raise ValueError("No JSON found")
            json_str = match.group(0)
            data = json.loads(json_str)
            analysis = SignalAnalysis(source_url=news.url, **data)
            return analysis
            
        except Exception as e:
            logger.warning(f"[Single Analyze] Parse Error: {e}")
            return None

    async def ensemble_analyze(self, news: NewsPayload) -> Optional[SignalAnalysis]:
        """
        ç”¨ä¸åŒæ¸©åº¦/æ¨¡å‹è·‘3æ¬¡,å–ä¸­ä½æ•°
        ç‰©ç†ç›´è§‰:å¤šæ¬¡æµ‹é‡æ±‚å¹³å‡å€¼
        """
        results = []
        temps = [0.1, 0.5, 0.7]  # ä¸‰ä¸ªæ¸©åº¦æ¡£ä½
        
        # å¹¶å‘æ‰§è¡Œå¤šæ¬¡åˆ†æ
        tasks = [self._single_analyze(news, temp) for temp in temps]
        results_raw = await asyncio.gather(*tasks)
        
        # è¿‡æ»¤å¤±è´¥çš„ç»“æœ
        results = [r for r in results_raw if r is not None]
        
        if not results:
            return None
            
        if len(results) >= 2:
            # å–ä¸­ä½æ•°scoreå’Œcertainty
            scores = [r.score for r in results]
            certainties = [r.certainty for r in results]
            
            # é€‰æ‹©æœ€è¯¦ç»†çš„ reasoning (æˆ–è€…æœ€é•¿çš„)
            best_reasoning = max(results, key=lambda x: len(x.reasoning)).reasoning
            
            # åˆå¹¶ç›¸å…³è‚¡ç¥¨ (å»é‡)
            all_stocks = set()
            for r in results:
                all_stocks.update(r.related_stocks)
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´ (å–æ‰€æœ‰ç»“æœçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ä½œä¸ºä¿å®ˆä¼°è®¡)
            all_ranges = [r.confidence_range for r in results if r.confidence_range]
            if all_ranges:
                min_conf = min(r[0] for r in all_ranges)
                max_conf = max(r[1] for r in all_ranges)
                final_conf_range = [min_conf, max_conf]
            else:
                final_conf_range = results[0].confidence_range

            return SignalAnalysis(
                source_url=news.url,
                score=int(np.median(scores)),
                certainty=int(np.median(certainties)),
                confidence_range=final_conf_range,
                reasoning=best_reasoning,
                related_stocks=list(all_stocks),
                time_horizon=results[0].time_horizon # å‡è®¾æ—¶é—´å°ºåº¦ä¸€è‡´ï¼Œæˆ–è€…åº”è¯¥æŠ•ç¥¨
            )
        
        return results[0]

    async def adversarial_validate(self, analysis: SignalAnalysis) -> float:
        """
        è®©æ¨¡å‹æ‰®æ¼”åæ–¹,æŒ‘æˆ˜åŸåˆ†æçš„æ¼æ´
        è¿”å›ä¿¡å¿ƒä¿®æ­£ç³»æ•° (0.5~1.0)
        """
        challenge_prompt = f"""
        åŸåˆ†æç»™å‡ºè¯„åˆ† {analysis.score}/10,ç†ç”±æ˜¯:
        {analysis.reasoning}
        
        è¯·ä½ ä½œä¸ºé­”é¬¼ä»£è¨€äºº,æŒ‡å‡ºè¿™ä¸ªåˆ†æå¯èƒ½å­˜åœ¨çš„3ä¸ªæœ€å¤§é—®é¢˜:
        1. å¿½ç•¥çš„åå‘å› ç´ 
        2. è¿‡åº¦è§£è¯»çš„éƒ¨åˆ†
        3. æ—¶é—´å°ºåº¦æ˜¯å¦åˆç†
        
        è¯·ä»”ç»†æ€è€ƒã€‚å¦‚æœè®¤ä¸ºåŸåˆ†ææœ‰ä¸¥é‡é”™è¯¯æˆ–é‡å¤§é—æ¼ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚
        """
        
        critique = await self.call_model(challenge_prompt, temp=0.7, max_tokens=1024)
        
        # ç®€å•è§£æ:å¦‚æœæå‡ºä¸¥é‡è´¨ç–‘,é™ä½certainty
        # è¿™é‡Œçš„åˆ¤æ–­é€»è¾‘æ¯”è¾ƒç®€å•ï¼Œå¯ä»¥åç»­ä¼˜åŒ–
        if "ä¸¥é‡" in critique or "é”™è¯¯" in critique or "å¿½ç•¥" in critique:
            return 0.7
        return 0.95

    async def slow_path_analyze(self, news: NewsPayload) -> Optional[SignalAnalysis]:
        """
        æ…¢é€šé“ï¼šæ·±åº¦æ€ç»´é“¾åˆ†æ (System 2 Reasoning)
        ç°åœ¨é›†æˆäº† Ensemble å’Œ Adversarial Validation
        """
        # 1. Ensemble Analysis
        analysis = await self.ensemble_analyze(news)
        if not analysis:
            return None
            
        # 2. Adversarial Validation
        # åªæœ‰å½“ä¿¡å·æ¯”è¾ƒå¼ºæ—¶æ‰å€¼å¾—è¿›è¡Œå¯¹æŠ—éªŒè¯ï¼ŒèŠ‚çœToken
        if abs(analysis.score) >= 5 and analysis.certainty >= 6:
             logger.info(f"ğŸ›¡ï¸ Running Adversarial Validation for {news.title[:20]}...")
             confidence_modifier = await self.adversarial_validate(analysis)
             
             # ä¿®æ­£ certainty
             original_certainty = analysis.certainty
             analysis.certainty = int(original_certainty * confidence_modifier)
             
             if analysis.certainty != original_certainty:
                 logger.info(f"ğŸ“‰ Certainty adjusted from {original_certainty} to {analysis.certainty} after adversarial check.")

        return analysis
