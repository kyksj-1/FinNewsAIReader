import aiohttp
import asyncio
import json
import re
import subprocess
from typing import Optional
from loguru import logger
from config.settings import settings
from core.schema import NewsPayload, SignalAnalysis


class LLMEngine:
    """
    æ¨ç†å¼•æ“ï¼šæ”¯æŒæœ¬åœ° Ollama å’Œäº‘ç«¯ DeepSeek
    """

    def __init__(self):
        limit = 50 if settings.LLM_PROVIDER == "deepseek" else settings.MAX_GPU_CONCURRENCY
        self.concurrency_lock = asyncio.Semaphore(limit)

    async def _get_gpu_temperature(self) -> Optional[int]:
        def runner() -> Optional[int]:
            try:
                result = subprocess.run(
                    [
                        "nvidia-smi",
                        "--query-gpu=temperature.gpu",
                        "--format=csv,noheader,nounits",
                    ],
                    capture_output=True,
                    text=True,
                    check=True,
                )
                line = result.stdout.strip().splitlines()[0].strip()
                return int(line)
            except Exception:
                return None

        return await asyncio.to_thread(runner)

    async def _wait_for_safe_temperature(self) -> None:
        limit = settings.GPU_TEMP_LIMIT
        resume = settings.GPU_TEMP_RESUME
        interval = settings.GPU_TEMP_CHECK_INTERVAL
        if limit <= 0 or interval <= 0:
            return
        if resume <= 0 or resume >= limit:
            resume = max(limit - 10, 0)
        temp = await self._get_gpu_temperature()
        if temp is None:
            return
        if temp < limit:
            return
        logger.warning(f"GPU temperature {temp}Â°C exceeds limit {limit}Â°C, waiting for cooldown")
        while True:
            await asyncio.sleep(interval)
            temp = await self._get_gpu_temperature()
            if temp is None:
                logger.warning("GPU temperature check failed during cooldown, resuming inference")
                return
            if temp <= resume:
                logger.info(f"GPU temperature {temp}Â°C is below resume threshold {resume}Â°C, resuming inference")
                return

    async def _call_deepseek(self, prompt: str, temp: float, max_tokens: int) -> str:
        """
        DeepSeek API è°ƒç”¨ (OpenAI å…¼å®¹åè®®)
        """
        url = f"{settings.DEEPSEEK_BASE_URL}/chat/completions"
        headers = {
            "Authorization": f"Bearer {settings.DEEPSEEK_API_KEY}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": settings.DEEPSEEK_MODEL_NAME,
            "messages": [
                {"role": "system", "content": "You are a professional financial quantitative analyst."},
                {"role": "user", "content": prompt},
            ],
            "temperature": temp,
            "max_tokens": max_tokens,
            "stream": False,
        }
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(url, headers=headers, json=payload, timeout=120) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        logger.error(f"[DeepSeek] Error {resp.status}: {error_text}")
                        return ""
                    data = await resp.json()
                    return data["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error(f"[DeepSeek] Connection Failed: {e}")
                return ""

    async def _call_ollama(self, prompt: str, temp: float, max_tokens: int) -> str:
        """
        æœ¬åœ° Ollama è°ƒç”¨
        """
        url = f"{settings.OLLAMA_BASE_URL}/api/generate"
        payload = {
            "model": settings.LOCAL_MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temp,
                "num_ctx": settings.CONTEXT_WINDOW,
                "num_predict": max_tokens,
            },
        }
        await self._wait_for_safe_temperature()
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(url, json=payload, timeout=60) as resp:
                    resp.raise_for_status()
                    data = await resp.json()
                    return data.get("response", "")
            except Exception as e:
                logger.error(f"[Ollama] Error: {e}")
                return ""

    async def call_model(self, prompt: str, temp: float, max_tokens: int = 2048) -> str:
        """
        ç»Ÿä¸€å…¥å£ï¼šæ ¹æ®é…ç½®åˆ†å‘
        """
        async with self.concurrency_lock:
            if settings.LLM_PROVIDER == "deepseek":
                return await self._call_deepseek(prompt, temp, max_tokens)
            return await self._call_ollama(prompt, temp, max_tokens)

    async def fast_path_filter(self, news: NewsPayload) -> bool:
        """
        å¿«é€šé“ï¼šåŸºäºæ ‡é¢˜çš„å¿«é€ŸäºŒåˆ†ç±» (High-Pass Filter)
        ä¿®å¤ç‰ˆï¼šä½¿ç”¨ä¸­æ–‡ Promptï¼Œé™ä½é˜ˆå€¼ï¼Œå¢åŠ  Debug æ—¥å¿—
        """
        # 1. æç®€è§„åˆ™ï¼šå¦‚æœæ ‡é¢˜åŒ…å«ç‰¹å®šç¡¬å…³é”®è¯ï¼Œç›´æ¥é€šè¿‡ï¼ˆæ—è·¯æœºåˆ¶ï¼‰
        # ç‰©ç†ç›´è§‰ï¼šæœ‰äº›ä¿¡å·å¤ªæ˜æ˜¾ï¼Œä¸éœ€è¦è¿‡æ¨¡å‹
        keywords = ["Aè‚¡", "è‚¡å¸‚", "äººæ°‘å¸", "å¤®è¡Œ", "ç¾è”å‚¨", "åˆ©å¥½", "åˆ©ç©º", "GDP", "CPI", "ç›‘ç®¡"
        "èŠ¯ç‰‡", "åŠå¯¼ä½“", "è´¢æŠ¥", "å¢æŒ", "å›è´­", "AI", "é‡‘è", "ç®—åŠ›", "åŠå¯¼ä½“",
        "æ²ªæŒ‡", "æ¿å—", "æ¦‚å¿µè‚¡", "è‚¡ç¥¨", "æ¶¨åœ", "è·Œåœ", "å›è°ƒ", "åå¼¹", "å¸‚åœºæƒ…ç»ª",
        "èèµ„", "è¯åˆ¸", "å¤§ç›˜", "æŒ‡æ•°", "æˆäº¤é¢", "åŒ—å‘", "å¤–èµ„", "ç‰¹æ–¯æ‹‰", "å®å¾·æ—¶ä»£"
        ]
        if any(k in news.title for k in keywords):
            logger.info(f"âš¡ [Fast Path] Keyword Bypass | {news.title[:20]}...")
            return True

        # 2. LLM åˆ¤åˆ«
        prompt = f"""
        ä½ æ˜¯Aè‚¡é‡åŒ–äº¤æ˜“å‘˜ã€‚åˆ¤æ–­ä»¥ä¸‹æ–°é—»æ ‡é¢˜æ˜¯å¦å±äº"é‡‘èã€å®è§‚ç»æµã€è‚¡å¸‚ã€ç§‘æŠ€ã€æ”¿ç­–"èŒƒç•´ã€‚
        
        æ ‡é¢˜ï¼š"{news.title}"
        
        å¦‚æœæ˜¯ï¼Œè¯·å›ç­”"æ˜¯"ã€‚
        å¦‚æœå®Œå…¨æ— å…³ï¼ˆå¦‚å¨±ä¹ã€ä½“è‚²ã€çº¯å…«å¦ã€å°å‹ç¤¾ä¼šäº‹ä»¶ç­‰ï¼‰ï¼Œè¯·å›ç­”"å¦"ã€‚
        åªå›ç­”ä¸€ä¸ªå­—ã€‚
        """
        res = await self.call_model(prompt, temp=settings.TEMP_FAST, max_tokens=64)
        clean_res = res.strip().upper()
        
        # ä¿®æ”¹ç‚¹ï¼šæ‰“å°åŸå§‹å›å¤ï¼Œçœ‹çœ‹å®ƒåˆ°åº•æƒ³è¯´ä»€ä¹ˆ
        logger.debug(f"Raw Model Response: {clean_res}")
        

        
        # 3. å®½æ¾åˆ¤åˆ«é€»è¾‘
        is_relevant = "æ˜¯" in clean_res or "Yes" in clean_res or "ç›¸å…³" in clean_res
        
        status = "Relevant" if is_relevant else "Noise"
        # å…³é”®ï¼šæ‰“å°å‡ºæ¨¡å‹åˆ°åº•è¯´äº†ä»€ä¹ˆï¼Œæ–¹ä¾¿è°ƒè¯•
        logger.info(
            f"ğŸ” [Fast Path] Model said: '{clean_res}' -> {status} | Title: {news.title[:30]}..."
        )
        return is_relevant

    async def slow_path_analyze(self, news: NewsPayload) -> Optional[SignalAnalysis]:
        """
        æ…¢é€šé“ï¼šæ·±åº¦æ€ç»´é“¾åˆ†æ (System 2 Reasoning)
        """
        # å¢åŠ  max_tokensï¼Œç»™æ€è€ƒç•™å‡ºç©ºé—´
        max_tokens_limit = 4096 
        safe_content = news.content[:6000] if news.content else ""
        
        prompt = f"""
        [Role]
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±é‡åŒ–ç ”ç©¶å‘˜ã€‚ä½ éœ€è¦åˆ†ææ–°é—»å¯¹Aè‚¡å¸‚åœºçš„å½±å“ã€‚å½“è®¯æ¯ä¸­å‡ºç°è‚¡ç¥¨åå­—çš„æ—¶å€™ï¼Œå¿…é¡»æ ¼å¤–æ³¨æ„ï¼è¯´æ˜è¿™ä¸ªè‚¡ç¥¨æ˜¯æœ‰æ¶ˆæ¯çš„ã€‚

        [Input News]
        {safe_content}

        [Instructions]
        1. **Deep Thinking (å…³é”®æ­¥éª¤)**: 
           åœ¨è¾“å‡º JSON ä¹‹å‰ï¼Œå¿…é¡»å…ˆåœ¨ä¸€ä¸ª <think> æ ‡ç­¾å†…è¿›è¡Œæ·±åº¦æ¨æ¼”ã€‚
           - åˆ†æäº‹ä»¶çš„ä¸€é˜¶å½±å“ï¼ˆç›´æ¥å—ç›Š/å—æŸï¼‰ã€‚
           - åˆ†æäºŒé˜¶å½±å“ï¼ˆä¾›åº”é“¾ã€ç«äº‰å¯¹æ‰‹ã€æ›¿ä»£å“ï¼‰ã€‚
           - ç»“åˆå½“å‰å®è§‚ç¯å¢ƒï¼ˆæµåŠ¨æ€§ã€æ”¿ç­–å‘¨æœŸï¼‰è¯„ä¼°ä¿¡å·å¼ºåº¦ã€‚
           - æ¨å¯¼æœ€ç»ˆçš„ Scoreã€‚

        2. **Output Format**:
           æ€è€ƒç»“æŸåï¼Œè¾“å‡ºä¸¥æ ¼çš„ JSONã€‚
           
        [Example Output]
        <think>
        è¿™é‡Œå†™ä½ çš„æ·±åº¦æ¨ç†è¿‡ç¨‹...
        1. äº‹ä»¶æ ¸å¿ƒæ˜¯...
        2. ä¼ å¯¼è·¯å¾„æ˜¯...
        3. å¸‚åœºé¢„æœŸåœ¨äº...
        </think>
        {{
            "reasoning": "æ€»ç»“ä¸Šè¿°æ€è€ƒçš„ç®€ç»ƒç»“è®º...",
            "score": 7,
            "certainty": 8,
            "related_stocks": ["sh.600XXX"],
            "time_horizon": "Medium"
        }}
        """
        # è°ƒé«˜ä¸€ç‚¹æ¸©åº¦ï¼Œå®ç°å‘æ•£æ€§
        raw_res = await self.call_model(prompt, temp=settings.TEMP_SLOW, max_tokens=max_tokens_limit)
        raw_res = raw_res.replace("```json", "").replace("```", "")
        try:
            match = re.search(r"\{.*\}", raw_res, re.DOTALL)
            if not match:
                raise ValueError("No JSON found")
            json_str = match.group(0)
            data = json.loads(json_str)
            analysis = SignalAnalysis(source_url=news.url, **data)
            return analysis
            
        except Exception as e:
            logger.warning(f"[Slow Path] Parse Error: {e}")
            return None
