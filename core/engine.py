import aiohttp
import asyncio
import json
import re
from typing import Optional
from loguru import logger
from config.settings import settings
from core.schema import NewsPayload, SignalAnalysis

class LLMEngine:
    """
    æ¨ç†å¼•æ“ï¼šç®¡ç† 4060 æ˜¾å­˜èµ„æºä¸æ¨¡å‹äº¤äº’
    """
    def __init__(self):
        self.api_url = f"{settings.OLLAMA_BASE_URL}/api/generate"
        # æ˜¾å­˜æ˜¯ç¨€ç¼ºèµ„æºï¼Œå¿…é¡»æ’é˜Ÿè®¿é—®
        self.gpu_lock = asyncio.Lock() if settings.MAX_GPU_CONCURRENCY == 1 else asyncio.Semaphore(settings.MAX_GPU_CONCURRENCY)

    async def _call_ollama(self, prompt: str, temp: float, max_tokens: int = 2048) -> str:
        """
        åº•å±‚ API è°ƒç”¨ï¼Œå— GPU é”ä¿æŠ¤
        """
        payload = {
            "model": settings.MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temp,
                "num_ctx": settings.CONTEXT_WINDOW,
                "num_predict": max_tokens
            }
        }

        async with self.gpu_lock: # <--- ç‰©ç†ç“¶é¢ˆï¼šæ˜¾å­˜é”
            async with aiohttp.ClientSession() as session:
                try:
                    async with session.post(self.api_url, json=payload, timeout=60) as resp:
                        resp.raise_for_status()
                        data = await resp.json()
                        return data.get("response", "")
                except Exception as e:
                    logger.error(f"Inference Failure: {e}")
                    return ""


     # ä¿®æ”¹ core/engine.py
    async def fast_path_filter(self, news: NewsPayload) -> bool:
        """
        å¿«é€šé“ï¼šåŸºäºæ ‡é¢˜çš„å¿«é€ŸäºŒåˆ†ç±» (High-Pass Filter)
        ä¿®å¤ç‰ˆï¼šä½¿ç”¨ä¸­æ–‡ Promptï¼Œé™ä½é˜ˆå€¼ï¼Œå¢åŠ  Debug æ—¥å¿—
        """
        # 1. æç®€è§„åˆ™ï¼šå¦‚æœæ ‡é¢˜åŒ…å«ç‰¹å®šç¡¬å…³é”®è¯ï¼Œç›´æ¥é€šè¿‡ï¼ˆæ—è·¯æœºåˆ¶ï¼‰
        # ç‰©ç†ç›´è§‰ï¼šæœ‰äº›ä¿¡å·å¤ªæ˜æ˜¾ï¼Œä¸éœ€è¦è¿‡æ¨¡å‹
        keywords = ["Aè‚¡", "è‚¡å¸‚", "äººæ°‘å¸", "å¤®è¡Œ", "ç¾è”å‚¨", "åˆ©å¥½", "åˆ©ç©º", "GDP", "CPI", "ç›‘ç®¡"
        "èŠ¯ç‰‡", "åŠå¯¼ä½“", "è´¢æŠ¥", "å¢æŒ", "å›è´­", "AI", "é‡‘è", "ç®—åŠ›", "åŠå¯¼ä½“",
        "æ²ªæŒ‡", "æ¿å—", "æ¦‚å¿µè‚¡", "è‚¡ç¥¨", "æ¶¨åœ", "è·Œåœ", "å›è°ƒ", "åå¼¹", "å¸‚åœºæƒ…ç»ª"]
        if any(k in news.title for k in keywords):
            logger.info(f"âš¡ [Fast Path] Keyword Bypass | {news.title[:20]}...")
            return True

        # 2. LLM åˆ¤åˆ«
        prompt = f"""
        ä½ æ˜¯Aè‚¡é‡åŒ–äº¤æ˜“å‘˜ã€‚åˆ¤æ–­ä»¥ä¸‹æ–°é—»æ ‡é¢˜æ˜¯å¦å±äº"é‡‘èã€å®è§‚ç»æµã€è‚¡å¸‚ã€ç§‘æŠ€ã€æ”¿ç­–"èŒƒç•´ã€‚
        
        æ ‡é¢˜ï¼š"{news.title}"
        
        å¦‚æœæ˜¯ï¼Œè¯·å›ç­”"æ˜¯"ã€‚
        å¦‚æœå®Œå…¨æ— å…³ï¼ˆå¦‚å¨±ä¹ã€ä½“è‚²ã€çº¯å…«å¦ã€å°å‹ç¤¾ä¼šäº‹ä»¶ç­‰ï¼‰ï¼Œè¯·å›ç­”"å¦"ã€‚
        åªå›ç­”ä¸€ä¸ªå­—ã€‚
        """
        
        # ç¨å¾®è°ƒé«˜ä¸€ç‚¹ tempï¼Œè®©å®ƒæ•¢äºå›ç­”
        res = await self._call_ollama(prompt, temp=0.1, max_tokens=5)
        
        # æ¸…æ´—è¾“å‡ºï¼šå»æ‰æ ‡ç‚¹å’Œç©ºæ ¼
        clean_res = res.strip().replace("ã€‚", "").replace(".", "")
        
        # 3. å®½æ¾åˆ¤åˆ«é€»è¾‘
        is_relevant = "æ˜¯" in clean_res or "Yes" in clean_res or "ç›¸å…³" in clean_res
        
        status = "Relevant" if is_relevant else "Noise"
        # å…³é”®ï¼šæ‰“å°å‡ºæ¨¡å‹åˆ°åº•è¯´äº†ä»€ä¹ˆï¼Œæ–¹ä¾¿è°ƒè¯•
        logger.info(f"ğŸ” [Fast Path] Model said: '{clean_res}' -> {status} | Title: {news.title[:30]}...")
        
        return is_relevant

    async def slow_path_analyze(self, news: NewsPayload) -> Optional[SignalAnalysis]:
        """
        æ…¢é€šé“ï¼šæ·±åº¦æ€ç»´é“¾åˆ†æ (System 2 Reasoning)
        """
        # å¢åŠ  max_tokensï¼Œç»™æ€è€ƒç•™å‡ºç©ºé—´
        max_tokens_limit = 4096 
        safe_content = news.content[:6000] if news.content else ""
        
        prompt = f"""
        [Role]
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±é‡åŒ–ç ”ç©¶å‘˜ã€‚ä½ éœ€è¦åˆ†ææ–°é—»å¯¹Aè‚¡å¸‚åœºçš„å½±å“ã€‚

        [Input News]
        {safe_content}

        [Instructions]
        1. **Deep Thinking (å…³é”®æ­¥éª¤)**: 
           åœ¨è¾“å‡º JSON ä¹‹å‰ï¼Œå¿…é¡»å…ˆåœ¨ä¸€ä¸ª <think> æ ‡ç­¾å†…è¿›è¡Œæ·±åº¦æ¨æ¼”ã€‚
           - åˆ†æäº‹ä»¶çš„ä¸€é˜¶å½±å“ï¼ˆç›´æ¥å—ç›Š/å—æŸï¼‰ã€‚
           - åˆ†æäºŒé˜¶å½±å“ï¼ˆä¾›åº”é“¾ã€ç«äº‰å¯¹æ‰‹ã€æ›¿ä»£å“ï¼‰ã€‚
           - ç»“åˆå½“å‰å®è§‚ç¯å¢ƒï¼ˆæµåŠ¨æ€§ã€æ”¿ç­–å‘¨æœŸï¼‰è¯„ä¼°ä¿¡å·å¼ºåº¦ã€‚
           - åƒè§£å†³ç‰©ç†æ–¹ç¨‹ä¸€æ ·ï¼Œæ¨å¯¼æœ€ç»ˆçš„ Scoreã€‚

        2. **Output Format**:
           æ€è€ƒç»“æŸåï¼Œè¾“å‡ºä¸¥æ ¼çš„ JSONã€‚
           
        [Example Output]
        <think>
        è¿™é‡Œå†™ä½ çš„æ·±åº¦æ¨ç†è¿‡ç¨‹...
        1. äº‹ä»¶æ ¸å¿ƒæ˜¯...
        2. ä¼ å¯¼è·¯å¾„æ˜¯...
        3. å¸‚åœºé¢„æœŸåœ¨äº...
        </think>
        {{
            "reasoning": "æ€»ç»“ä¸Šè¿°æ€è€ƒçš„ç®€ç»ƒç»“è®º...",
            "score": 7,
            "certainty": 8,
            "related_stocks": ["sh.600XXX"],
            "time_horizon": "Medium"
        }}
        """
        
        # è°ƒé«˜ä¸€ç‚¹ Temperatureï¼Œå¢åŠ æ€ç»´çš„å‘æ•£æ€§
        raw_res = await self._call_ollama(prompt, temp=0.8, max_tokens=max_tokens_limit)
        
        try:
            # è§£æé€»è¾‘å‡çº§ï¼šå…ˆæå– JSON
            match = re.search(r"\{.*\}", raw_res, re.DOTALL)
            if not match:
                raise ValueError("No JSON found")
            
            json_str = match.group(0)
            data = json.loads(json_str)
            
            # å¯é€‰ï¼šå¦‚æœä½ æƒ³æŠŠ <think> å†…å®¹ä¹Ÿå­˜ä¸‹æ¥ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ­£åˆ™æå–
            # think_content = re.search(r"<think>(.*?)</think>", raw_res, re.DOTALL)
            
            analysis = SignalAnalysis(source_url=news.url, **data)
            return analysis
            
        except Exception as e:
            logger.warning(f"[Slow Path] Parse Error: {e}")
            return None
