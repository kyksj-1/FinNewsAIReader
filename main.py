import asyncio
import sys
import aiofiles
from loguru import logger
from config.settings import settings
from core.crawler import AsyncCrawler
from core.engine import LLMEngine
from core.schema import NewsPayload, SignalAnalysis
# main.py å¤´éƒ¨å¢åŠ å¯¼å…¥
from core.monitor import NewsMonitor

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(sys.stderr, level="INFO")
logger.add(settings.LOG_DIR / "finnews_master.log", rotation="10 MB", level="DEBUG")

class FinNewsPipeline:
    def __init__(self):
        self.crawler = AsyncCrawler()
        self.engine = LLMEngine()
        self.queue = asyncio.Queue(maxsize=100) # ç¼“å†²åŒºå¤§å°
        
    async def producer(self, urls: list[str]):
        """
        ç”Ÿäº§è€…ï¼šè´Ÿè´£æŠ“å–æ•°æ®å¹¶æ”¾å…¥é˜Ÿåˆ—
        """
        for url in urls:
            news = await self.crawler.process_url(url)
            if news:
                await self.queue.put(news)
        
        # æ”¾ç½®ç»“æŸå“¨å…µ
        await self.queue.put(None)
        logger.info("ğŸ“¡ Producer finished fetching all URLs.")

    async def consumer(self):
        """
        æ¶ˆè´¹è€…ï¼šä»é˜Ÿåˆ—å–æ•°æ®ï¼Œè¿›è¡Œ LLM åŒæµå¤„ç†
        """
        while True:
            news = await self.queue.get()
            if news is None:
                self.queue.task_done()
                break 
            
            try:
                # === è¯Šæ–­æ’æ¡©ï¼šå¼ºåˆ¶ä¿å­˜ Raw Data ===
                # åªè¦æŠ“åˆ°äº†ï¼Œå…ˆå­˜ä¸‹æ¥ï¼Œè¯æ˜æˆ‘ä»¬æ¥è¿‡
                raw_filename = f"raw_{int(asyncio.get_event_loop().time() * 1000)}.txt"
                raw_path = settings.DATA_RAW_DIR / raw_filename
                
                # ç®€å•çš„å†™æ–‡ä»¶æ“ä½œ
                try:
                    async with aiofiles.open(raw_path, mode='w', encoding='utf-8') as f:
                        await f.write(f"URL: {news.url}\nTITLE: {news.title}\nCONTENT:\n{news.content}")
                except Exception as save_err:
                    logger.error(f"Failed to save raw: {save_err}")
                # =================================

                # 1. Fast Path
                if await self.engine.fast_path_filter(news):
                    logger.info(f"âš¡ Entering Slow Path: {news.title[:30]}...")
                    analysis = await self.engine.slow_path_analyze(news)
                    
                    if analysis:
                        await self.save_result(analysis)
                        logger.success(f"ğŸ¯ Signal Extracted: Score {analysis.score} | {analysis.related_stocks}")
                
                # å“ªæ€•æ˜¯ Noiseï¼Œå› ä¸ºå‰é¢å·²ç» save raw äº†ï¼Œè¿™é‡Œå°±ä¸éœ€è¦é¢å¤–æ“ä½œäº†

            except Exception as e:
                logger.exception(f"Pipeline Error processing {news.url}: {e}")
            finally:
                self.queue.task_done()

                
    async def save_result(self, analysis: SignalAnalysis):
        """
        ä¿å­˜ç»“æœåˆ° JSONL
        """
        file_path = settings.DATA_SIGNAL_DIR / f"signals_{analysis.time_horizon}.jsonl"
        async with aiofiles.open(file_path, mode='a', encoding='utf-8') as f:
            await f.write(analysis.model_dump_json() + "\n")

    async def run(self, urls: list[str]):
        logger.info("ğŸš€ FinNewsMasterV1 System Launching...")
        logger.info(f"HARDWARE: Max GPU Concurrency = {settings.MAX_GPU_CONCURRENCY}")
        
        # å¹¶å‘è¿è¡Œç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…
        producer_task = asyncio.create_task(self.producer(urls))
        consumer_task = asyncio.create_task(self.consumer())
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await asyncio.gather(producer_task, consumer_task)
        logger.info("âœ… All tasks completed.")


async def main_loop():
    logger.info("ğŸš€ FinNewsMasterV1: AUTO-PILOT MODE ENGAGED")
    
    pipeline = FinNewsPipeline()
    monitor = NewsMonitor()
    
    # å¯åŠ¨æ¶ˆè´¹è€…ä»»åŠ¡ (åå°ä¸€ç›´è¿è¡Œï¼Œç­‰å¾…å¤„ç†æ•°æ®)
    consumer_task = asyncio.create_task(pipeline.consumer())
    
    try:
        while True:
            # 1. é›·è¾¾æ‰«æ
            logger.info("ğŸ“¡ Scanning markets for new intelligence...")
            new_urls = await monitor.harvest()
            
            if new_urls:
                # 2. åªæœ‰å‘ç°æ–°é“¾æ¥æ—¶ï¼Œæ‰å¯åŠ¨ç”Ÿäº§è€…æ”¾å…¥é˜Ÿåˆ—
                logger.info(f"ğŸ“¥ Feeding {len(new_urls)} URLs to pipeline...")
                await pipeline.producer(new_urls)
            else:
                logger.info("ğŸ’¤ No new signals. Standing by.")
            
            # 3. å†·å´æ—¶é—´ (æ¯”å¦‚æ¯ 60 ç§’æ‰«ä¸€æ¬¡ï¼Œé¿å…è¢«å° IP)
            await asyncio.sleep(30)
            
    except KeyboardInterrupt:
        logger.warning("ğŸ›‘ Manual Stop Signal Received.")
    finally:
        # ä¼˜é›…å…³é—­ï¼šå‘é€ç©ºä¿¡å·ç»™æ¶ˆè´¹è€…ï¼Œè®©å®ƒä¸‹ç­
        await pipeline.queue.put(None)
        await consumer_task


if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    asyncio.run(main_loop())
